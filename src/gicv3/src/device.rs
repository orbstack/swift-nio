// References:
// - Learn the architecture - Generic Interrupt Controller v3 and v4, Overview (version 3.2)
// - Arm Generic Interrupt Controller Architecture Specification GIC architecture version 3 and version 4
// - https://wiki.osdev.org/Generic_Interrrupt_Controller
// - https://wiki.osdev.org/Generic_Interrupt_Controller_versions_3_and_4

// === Definitions === //

use counter::RateCounter;
use std::{
    collections::VecDeque,
    sync::{
        atomic::{AtomicBool, AtomicU8, Ordering::*},
        Arc, RwLock,
    },
};
use utils::Mutex;

use rustc_hash::FxHashMap;

use crate::mmio_util::BitPack;

counter::counter! {
    COUNT_SPI in "gic.irq.spi": RateCounter = RateCounter::new(FILTER);
    COUNT_PPI in "gic.irq.ppi": RateCounter = RateCounter::new(FILTER);
}

/// The ID of an interrupt target that the GIC can route. The interrupt kind is determined by its ID.
/// Each ID corresponds to an *interrupt source*.
#[derive(Debug, Copy, Clone, Hash, Eq, PartialEq)]
pub struct InterruptId(pub u32);

impl InterruptId {
    pub const BITS: u32 = 10;

    // For more information about this classification, see section 2.2 of the specification.
    pub fn kind(self) -> InterruptKind {
        match self.0 {
            0..=15 => InterruptKind::SoftwareGenerated,
            16..=31 => InterruptKind::PrivatePeripheral,
            1056..=1119 => InterruptKind::PrivatePeripheral,
            32..=1019 => InterruptKind::SharedPeripheral,
            4096..=5119 => InterruptKind::SharedPeripheral,
            // These are special interrupt numbers but Linux configures them on the distributor so
            // these have to be advertised SPIsâ€”even if nothing ever gets delivered to them.
            1020..=1023 => InterruptKind::SharedPeripheral,
            1024..=8191 => unreachable!("reserved"),
            _ => unreachable!("LPI interrupt ID is too big"),
        }
    }
}

/// The kind of an interrupt. Different interrupt kinds have different handling behavior.
#[derive(Debug, Copy, Clone, Eq, PartialEq)]
pub enum InterruptKind {
    /// **Shared Peripheral Interrupt**
    ///
    /// These are the IRQs that most internal and external peripherals generate to interact with the
    /// system.
    ///
    /// This is a peripheral interrupt that the Distributor can route to a specified PE that can
    /// handle the interrupt, or to a PE that is one of a group of PEs in the system that has been
    /// configured to accept this type of interrupt:
    ///
    /// - SPIs can be Group 0 interrupts, Secure Group 1 interrupts, or Non-secure Group 1 interrupts.
    /// - SPIs can support either edge-triggered or level-sensitive behavior.
    /// - SPIs are never routed using an ITS.
    /// - SPIs have an active state and therefore require explicit deactivation.
    ///
    SharedPeripheral,

    /// **Private Peripheral Interrupt**
    ///
    /// This is a peripheral interrupt that targets a single, specific PE, and different PEs can use
    /// the same interrupt number to indicate different events:
    /// - PPIs can be Group 0 interrupts, Secure Group 1 interrupts, or Non-secure Group 1 interrupts.
    /// - PPIs can support either edge-triggered or level-sensitive behavior.
    /// - PPIs are never routed using an ITS.
    /// - PPIs have an active state and therefore require explicit deactivation.
    ///
    /// ## Note
    ///
    /// Commonly, it is expected that PPIs are used by different instances of the same interrupt
    /// source on each PE, thereby allowing a common interrupt number to be used for PE-specific
    /// events, such as the interrupts from a private timer.
    PrivatePeripheral,

    /// **Software Generated Interrupt**
    ///
    /// SGIs are typically used for inter-processor communication, and are generated by a write to
    /// an SGI register in the GIC:
    ///
    /// - SGIs can be Group 0 interrupts, Secure Group 1 interrupts, or Non-secure Group 1 interrupts.
    /// - SGIs have edge-triggered behavior.
    /// - SGIs are never routed using an ITS.
    /// - SGIs have an active state and therefore require explicit deactivation.
    ///
    SoftwareGenerated,
}

impl InterruptKind {
    pub fn is_shared(self) -> bool {
        matches!(self, Self::SharedPeripheral)
    }

    pub fn is_local(self) -> bool {
        !self.is_shared()
    }
}

/// An identifier for a CPU which gives information about its location.
///
/// For example, these could formatted as follows:
///
/// ```plain_text
/// <group of groups>.<group of processors>.<processor>.<core>
/// ```
///
/// ...although it's really up to the processor.
#[derive(Debug, Copy, Clone, Hash, Eq, PartialEq)]
pub struct Affinity(pub [u8; 4]);

impl Affinity {
    pub fn as_typer_id(self) -> u32 {
        BitPack::default()
            // In `GICR_TYPER`...
            // Bits [63:56] provide Aff3, the Affinity level 3 value for the Redistributor.
            .set_range(24, 31, self.0[3].into())
            // Bits [55:48] provide Aff2, the Affinity level 2 value for the Redistributor.
            .set_range(16, 23, self.0[2].into())
            // Bits [47:40] provide Aff1, the Affinity level 1 value for the Redistributor.
            .set_range(8, 15, self.0[1].into())
            // Bits [39:32] provide Aff0, the Affinity level 0 value for the Redistributor.
            .set_range(0, 7, self.0[0].into())
            .0
    }
}

/// Whether a given interrupt is edge-triggered or level-sensitive.
#[derive(Debug, Copy, Clone, Eq, PartialEq)]
pub enum InterruptTrigger {
    /// An interrupt that is edge-triggered has the following property:
    ///
    /// - It is asserted on detection of a rising edge of an interrupt signal and then, regardless of
    ///   the state of the signal, remains asserted until the interrupt is acknowledged by software.
    ///
    EdgeTriggered,

    /// An interrupt that is level-sensitive has the following properties:
    ///
    /// - It is asserted whenever the interrupt signal level is active, and deasserted whenever the
    ///   level is not active.
    /// - It is explicitly deasserted by software.
    ///
    LevelSensitive,
}

impl InterruptTrigger {
    pub fn from_two_bit_repr(val: u32) -> Self {
        match val {
            0b10 => InterruptTrigger::EdgeTriggered,
            0b00 => InterruptTrigger::LevelSensitive,
            _ => unreachable!(),
        }
    }

    pub fn to_two_bit_repr(self) -> u32 {
        match self {
            InterruptTrigger::EdgeTriggered => 0b10,
            InterruptTrigger::LevelSensitive => 0b00,
        }
    }
}

/// The unique identifier for a *processing element*. These can be thought of as hardware threads,
/// each with their own set of registers.
#[derive(Debug, Copy, Clone, Hash, Eq, PartialEq)]
pub struct PeId(pub u64);

/// The number of bits used to encode a priority value, from 4 to 8.
// N.B. We have to set this to something less than `6` to ensure that `gic_cpu_sys_reg_init` doesn't
// write to `ICC_AP1R1_EL1`, which causes an unrecoverable EL exception for some reason.
pub const PRIORITY_BITS: u64 = 4;

pub trait GicV3EventHandler {
    fn kick_vcpu_for_irq(&mut self, pe: PeId);

    fn kick_vcpu_for_pvlock(&mut self, pe: PeId);

    fn handle_custom_eoi(&mut self, pe: PeId, int_id: InterruptId);

    fn get_affinity(&mut self, pe: PeId) -> Affinity;
}

// === Device === //

#[derive(Debug, Default)]
pub struct GicV3 {
    pub pe_states: RwLock<FxHashMap<PeId, PeState>>,
    pub affinity_to_pe: RwLock<FxHashMap<Affinity, PeId>>,
    pub shared_int_configs: RwLock<FxHashMap<InterruptId, InterruptConfig>>,
    pub local_int_configs: RwLock<FxHashMap<(PeId, InterruptId), InterruptConfig>>,
    pub shared_int_queues: RwLock<FxHashMap<InterruptId, GlobalInterruptState>>,
    pub enable_are: AtomicBool,
    pub enable_grp_1: AtomicBool,
    pub enable_grp_0: AtomicBool,
}

impl GicV3 {
    pub fn affinity_to_pe(&self, affinity: Affinity) -> Option<PeId> {
        self.affinity_to_pe.read().unwrap().get(&affinity).copied()
    }

    pub fn read_interrupt_config(&self, pe: PeId, id: InterruptId) -> InterruptConfig {
        if id.kind() == InterruptKind::PrivatePeripheral {
            self.local_int_configs
                .read()
                .unwrap()
                .get(&(pe, id))
                .cloned()
                .unwrap_or_else(|| InterruptConfig::new(pe, id))
        } else {
            self.shared_int_configs
                .read()
                .unwrap()
                .get(&id)
                .cloned()
                .unwrap_or_else(|| InterruptConfig::new(pe, id))
        }
    }

    pub fn write_interrupt_config<R>(
        &self,
        pe: PeId,
        id: InterruptId,
        f: impl FnOnce(&mut InterruptConfig) -> R,
    ) -> R {
        if id.kind() == InterruptKind::PrivatePeripheral {
            f(self
                .local_int_configs
                .write()
                .unwrap()
                .entry((pe, id))
                .or_insert_with(|| InterruptConfig::new(pe, id)))
        } else {
            f(self
                .shared_int_configs
                .write()
                .unwrap()
                .entry(id)
                .or_insert_with(|| InterruptConfig::new(pe, id)))
        }
    }

    pub fn pe_state<H: GicV3EventHandler, R>(
        &self,
        handler: &mut H,
        pe: PeId,
        f: impl FnOnce(&mut H, &PeState) -> R,
    ) -> R {
        if let Some(pe) = self.pe_states.read().unwrap().get(&pe) {
            return f(handler, pe);
        }

        self.pe_states
            .write()
            .unwrap()
            .entry(pe)
            .or_insert_with(|| {
                let affinity = handler.get_affinity(pe);
                let replaced = self.affinity_to_pe.write().unwrap().insert(affinity, pe);

                if let Some(replaced) = replaced {
                    panic!(
                        "multiple PEs found with affinity {affinity:?}: {pe:?} and {replaced:?}"
                    );
                }

                tracing::trace!("Found new PE: {pe:?} -> {affinity:?}");

                PeState::new(affinity)
            });

        f(handler, &self.pe_states.read().unwrap()[&pe])
    }

    pub fn shared_int_queue<R>(
        &self,
        int_id: InterruptId,
        f: impl FnOnce(&GlobalInterruptState) -> R,
    ) -> R {
        if let Some(state) = self.shared_int_queues.read().unwrap().get(&int_id) {
            return f(state);
        }

        self.shared_int_queues
            .write()
            .unwrap()
            .entry(int_id)
            .or_default();

        f(self.shared_int_queues.read().unwrap().get(&int_id).unwrap())
    }

    pub fn send_spi(
        &self,
        handler: &mut impl GicV3EventHandler,
        pe: Option<PeId>,
        int_id: InterruptId,
    ) {
        assert_eq!(int_id.kind(), InterruptKind::SharedPeripheral);
        COUNT_SPI.count();

        // Determine target PE
        let pe_states = self.pe_states.read().unwrap();
        let (&pe, pe_state) = match &pe {
            Some(pe) => (pe, pe_states.get(pe).unwrap()),
            // HACK: We just send the SPI to the first PE we find but I don't think that's
            // spec-compliant, technically? It looks like all PEs can accept SPIs, though, so it's
            // probably fine for Linux.
            None => pe_states.iter().next().unwrap(),
        };

        // Ensure that no one else is asserting this SPI on a different PE. If they are, we need to
        // get in line because, otherwise, Linux will ignore the concurrent SPI.
        self.shared_int_queue(int_id, |queue| {
            let mut deliver_to = queue.deliver_to.lock().unwrap();

            if deliver_to.is_empty() {
                // No one else is handling this SPI right now so we can deliver ours.
                deliver_to.push_back(pe);
            } else {
                // Another PE is handling this SPI. Add that PE to the backlog but eliminate duplicates
                // *within the backlog section of the array* to avoid live-lock if the interrupt handler
                // is handling interrupts slower than we produce them. We can eliminate interrupts in the
                // backlog since the state we're trying to notify the CPU of is already going to be observed
                // when the first instance of this interrupt is delivered. We ignore the front of the
                // iterator since the handler may have missed the state we just set.
                if !deliver_to.iter().skip(1).any(|&v| v == pe) {
                    deliver_to.push_back(pe);
                    tracing::trace!("SPI backlog for {int_id:?} = {}", deliver_to.len());
                }
            }

            // Otherwise, deliver the SPI!
            Self::deliver_interrupt_to_pe(handler, pe, pe_state, int_id, true);
        });
    }

    pub fn acknowledge_spi(&self, handler: &mut impl GicV3EventHandler, int_id: InterruptId) {
        self.shared_int_queue(int_id, |queue| {
            let mut deliver_to = queue.deliver_to.lock().unwrap();
            deliver_to.pop_front();

            let Some(&new_pe) = deliver_to.front() else {
                return;
            };

            self.pe_state(handler, new_pe, |handler, new_pe_state| {
                Self::deliver_interrupt_to_pe(handler, new_pe, new_pe_state, int_id, true);
            });
        })
    }

    pub fn send_ppi(
        &self,
        handler: &mut impl GicV3EventHandler,
        pe: PeId,
        int_id: InterruptId,
        is_local: bool,
    ) {
        assert_eq!(int_id.kind(), InterruptKind::PrivatePeripheral);
        COUNT_PPI.count();

        self.pe_state(handler, pe, |handler, pe_state| {
            Self::deliver_interrupt_to_pe(handler, pe, pe_state, int_id, !is_local);
        });
    }

    pub fn deliver_interrupt_to_pe(
        handler: &mut impl GicV3EventHandler,
        pe: PeId,
        pe_state: &PeState,
        int_id: InterruptId,
        needs_kick: bool,
    ) {
        // Check whether the target PE can receive the interrupt
        // TODO
        if !pe_state.is_enabled.load(Relaxed) {
            tracing::trace!(
                "Ignoring interrupt {int_id:?} of type {:?} to {pe:?}",
                int_id.kind()
            );
        }

        // Push it to the queue
        // TODO: Prioritize these
        tracing::trace!(
            "Delivering interrupt {int_id:?} of type {:?} to {pe:?}",
            int_id.kind(),
        );

        // we must push to the back of the queue, and not the front, in order for PV GIC to work
        // otherwise the next peeked interrupt could change between vmenter and vmexit (which is when we prcoess the deferred IAR1)
        // due to PV GIC, if we ever dedupe interrupts, it's fine -- we just need to allow 2 copies of each interrupt (e.g. 2 bitmasks) to make sure that any new interrupt delivered between when guest was supposed to IAR1, and when it was supposed to EOIR1, is not missed (because normally IAR1 dequeues and would make it no longer a duplicate when re-asserted)
        pe_state
            .int_state
            .lock()
            .unwrap()
            .push_interrupt_inner(int_id);

        if needs_kick {
            handler.kick_vcpu_for_irq(pe);
        }
    }
}

#[derive(Debug, Clone)]
pub struct InterruptConfig {
    pub trigger: InterruptTrigger,
    pub disabled: bool,
    pub not_forwarded: bool,
    pub affinity: Affinity,
}

impl InterruptConfig {
    pub fn new(_pe: PeId, _id: InterruptId) -> Self {
        Self {
            // FIXME: This might not be correct.
            trigger: InterruptTrigger::EdgeTriggered,

            // 12.9.24: On a GIC reset, this field resets to 0.
            disabled: false,

            // 12.9.7: On a GIC reset, this field resets to an architecturally UNKNOWN value.
            not_forwarded: false,

            // 12.9.22: On a GIC reset, this field resets to an architecturally UNKNOWN value.
            affinity: Affinity([0; 4]),
        }
    }
}

#[derive(Debug)]
pub struct PeState {
    pub affinity: Affinity,
    pub min_priority: AtomicU8,
    pub is_enabled: AtomicBool,
    pub int_state: Arc<Mutex<PeInterruptState>>,
}

impl PeState {
    pub fn new(affinity: Affinity) -> Self {
        Self {
            affinity,
            min_priority: AtomicU8::new(0),
            is_enabled: AtomicBool::new(false),
            int_state: Arc::new(Mutex::new(PeInterruptState {
                pending_interrupts: VecDeque::new(),
                active_interrupt: None,
            })),
        }
    }
}

#[derive(Debug)]
pub struct PeInterruptState {
    pub pending_interrupts: VecDeque<InterruptId>,
    pub active_interrupt: Option<InterruptId>,
}

impl PeInterruptState {
    pub fn is_irq_line_asserted(&self) -> bool {
        !self.pending_interrupts.is_empty() || self.active_interrupt.is_some()
    }

    #[allow(clippy::manual_map)] // Keeps things clear
    pub fn get_pending_irq(&self) -> Option<InterruptId> {
        // Allow peeking at the next (active/pending) interrupt that IAR_EL1 (ack) would return, for
        // PV GIC
        if let Some(active_interrupt) = self.active_interrupt {
            Some(active_interrupt)
        } else {
            self.pending_interrupts.front().copied()
        }
    }

    pub fn push_local_interrupt(&mut self, int_id: InterruptId) {
        // Shared interrupts have to pass through the GIC
        debug_assert!(int_id.kind().is_local());
        self.push_interrupt_inner(int_id);
    }

    fn push_interrupt_inner(&mut self, int_id: InterruptId) {
        // If there's already more than 1 interrupt pending (uncommon case), make sure we only push
        // at most one duplicate. Any additional ones are just bad for performance, and searching
        // here is definitely worth the saved vmexits. For perf, avoid the scan unless there are
        // already several pending interrupts.
        if self.pending_interrupts.len() > 8 {
            let mut count = 0;
            for existing_int in &self.pending_interrupts {
                if existing_int == &int_id {
                    count += 1;
                    if count > 1 {
                        return;
                    }
                }
            }
        }

        self.pending_interrupts.push_back(int_id);
    }
}

#[derive(Debug, Default)]
pub struct GlobalInterruptState {
    // TODO: Do not mutex
    pub deliver_to: Mutex<VecDeque<PeId>>,
}
